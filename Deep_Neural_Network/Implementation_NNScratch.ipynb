{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of NeuralNetwork\n",
    "\n",
    "    -Hidden layer:2 and input and output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self,input_size,layers_dim,output_size):\n",
    "        np.random.seed(0)\n",
    "        \n",
    "        models = {}  #it store the weight and bias of every layer(hiddden,input and output layer)\n",
    "        \n",
    "        #Input layer and bias--1st hidden layer weights\n",
    "        models[\"W1\"]=np.random.randn(input_size,layers_dim[0]) #(n X l0)\n",
    "        models[\"b1\"]=np.zeros((1,layers_dim[0]))#(1 X l0)\n",
    "        \n",
    "        #1stHL-2ndHL wts and bias\n",
    "        models[\"W2\"] = np.random.randn(layers_dim[0],layers_dim[1])#(l0 X l1)\n",
    "        models[\"b2\"]=np.zeros((1,layers_dim[2]))#(1 X l1)\n",
    "        \n",
    "        #2ndHL-output layer wts\n",
    "        models[\"W3\"] = np.random.randn(layers_dim[1],output_size)#(l1 X k)\n",
    "        models[\"b3\"]=np.zeros((1,output_size))#(1 X k)\n",
    "        \n",
    "        self.models=models\n",
    "        self.activations_=None\n",
    "        \n",
    "    @staticmethod    \n",
    "    def softmax(z):\n",
    "        exp_z=np.exp(z)  #elementwise operation #vector\n",
    "        \n",
    "        return exp_z/np.sum(exp_z,axis=1,keepdims=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        W1 , W2 , W3 = self.models[\"W1\"] , self.models[\"W2\"] , self.models[\"W3\"]\n",
    "        b1 , b2 , b3 = self.models[\"b1\"] , self.models[\"b2\"] , self.models[\"b3\"]\n",
    "        \n",
    "        Z1 = np.dot(x,W1) + b1    #(m x n) X (n X l0) -- (m X l0)\n",
    "        A1 = np.tanh(Z1)\n",
    "        \n",
    "        Z2 = np.dot(A1,W2) + b2   #(m x l0) X (l0 X l1) -- (m X l1)\n",
    "        A2 = np.tanh(Z2)\n",
    "        \n",
    "        Z3 = np.dot(A2,W3) + b3    #(m x l1) X (l1 X l2) -- (m X l2)\n",
    "        Y_ = NeuralNetwork.softmax(Z3)\n",
    "        \n",
    "        self.activations_ = A1 , A2 , Y_\n",
    "        \n",
    "        return Y_\n",
    "    \n",
    "    def backward(self,x,y,alpha=0.001): #Here y is one hot vector of (m X k)\n",
    "        \n",
    "        W1 , W2 , W3 = self.models[\"W1\"] , self.models[\"W2\"] , self.models[\"W3\"]\n",
    "        b1 , b2 , b3 = self.models[\"b1\"] , self.models[\"b2\"] , self.models[\"b3\"]\n",
    "        \n",
    "        A1 , A2 , Y_ = self.activations_\n",
    "        \n",
    "        \n",
    "        # 1)_Gradient Calculation \n",
    "        \n",
    "        #loss propagate from output layer to 2nd HL\n",
    "        delta_3 = Y_ - y  #(m X k)\n",
    "        dW3 = np.dot(A2.T,delta_3) #A2.T--(l1 X m) #delta_3--(m X k) #(A2.T X delta_3)--(dW3)--(l1 X k)\n",
    "        db3 = np.sum(delta_3,axis=0,keepdims=True)/x.shape[0] #delta_3--(m X k) db3--(1Xk)\n",
    "        \n",
    "        #(1-A2^2) is derivative of tanh function \n",
    "        #loss propagate from 2nd HL to 1st HL\n",
    "        delta_2 = delta_3.dot(W3.T) * (1-np.square(A2)) #((m X k) X (k X l1)) * (m Xl1))---(m X l1)\n",
    "        dw2 = np.dot(A1.T,delta_2)# ((l0 X m) X (m X l1)) --- (l0 X l1)\n",
    "        db2 = np.sum(delta_2,axis=0,keepdims=True)/x.shape[0] #(m X l1)---(1 X l1)\n",
    "        \n",
    "        #loss propagate from 1st HL to input layer \n",
    "        delta_1 = delta_2.dot(W2.T) * (1-np.square(A1)) #( ((m X l1) X (l1 X l0)) * (m X l0) ) --- (m X l0)\n",
    "        dw1 = np.dot(x.T,delta_1) #( (n X m) X (m X l0) ) --- (n X l0)\n",
    "        db1 = np.sum(delta_1,axis=0,keepdims=True)/x.shape[0] #(m X l0) --- (1 X l0)\n",
    "        \n",
    "        # 2)_Update Weights\n",
    "        \n",
    "        self.models[\"w1\"] -= alpha * dw1\n",
    "        self.models[\"w2\"] -= alpha * dw2\n",
    "        self.models[\"w3\"] -= alpha * dw3\n",
    "        \n",
    "        self.models[\"b1\"] -= alpha * db1\n",
    "        self.models[\"b2\"] -= alpha * db2\n",
    "        self.models[\"b3\"] -= alpha * db3\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \n",
    "        return np.argmax(self.forward(x),axis=1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss(y_label,y_predict):#y_label is one hot vector means only one idx is 1(repr. class)\n",
    "                                 # and other is 0 -----Categorical Cross Entropy E(m example)\n",
    "        return -np.mean(y_label*np.log(y_predict)) #np.sum and np.mean both are right\n",
    "    \n",
    "    @staticmethod\n",
    "    def one_hot_vector(y,k):#k=len(Unique class)\n",
    "        \n",
    "        y_ohv=np.zeros((y.shape[0],k))\n",
    "        y_ohv[np.arange(y.shape[0]),y]=1\n",
    "        \n",
    "        return y_ohv\n",
    "    \n",
    "    @staticmethod\n",
    "    def fit(X,Y,alpha=0.001,epoch=50):\n",
    "        \n",
    "        loss=[]\n",
    "        \n",
    "        # 1)-Create the one hot vector\n",
    "        \n",
    "        k=len(np.unique(Y))\n",
    "        y_ohv=NeuralNetwork.one_hot_vector(Y,k)\n",
    "        \n",
    "        #2)-Calculate loss of every forward propagate and backpropagate the loss \n",
    "        \n",
    "        for i in range(epoch):\n",
    "            y_=self.forward(X)\n",
    "            l=self.loss(Y,y_)\n",
    "            loss.append(l)\n",
    "            self.backward(X,Y,alpha)\n",
    "            \n",
    "            #3)print the loss of every 5 epoch\n",
    "            \n",
    "            if i%5==0:\n",
    "                print(\"{} epoch --> Loss is {} \".format(i,l))\n",
    "    \n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def accuracy(self,X,Y):\n",
    "        pred=self.predict(X)\n",
    "        return np.sum(Y==pred)/X.shape[0]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.NeuralNetwork.softmax(z)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNetwork(4,np.array([4,3,4]),3).softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add Implementation_NNScratch.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git commit -m\"NN 2 hidden layer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
