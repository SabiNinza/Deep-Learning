{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VKWVuBtbnrb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import PyPDF2\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0xs6f4f3bnri"
   },
   "outputs": [],
   "source": [
    "# import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXA5HSJxbnro"
   },
   "outputs": [],
   "source": [
    "# !pip3 install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEb1_lv6bnrv",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QuNivvlZbnr2"
   },
   "source": [
    "### Pdf extact text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EExYmlTabnr4"
   },
   "outputs": [],
   "source": [
    "pdf_obj = open(\"One_Indian_Girl_-_Chetan_Bhagat-Redicals.pdf\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7iw2uhN0bnr-"
   },
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfFileReader(pdf_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xl8ssl1lbnsE",
    "outputId": "01cef27f-4f55-40f2-8a73-05044a2b900a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_of_pg = reader.numPages\n",
    "print(No_of_pg)\n",
    "pg_obj = reader.getPage(50)\n",
    "pg_obj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsUErlHBbnsM"
   },
   "outputs": [],
   "source": [
    "# text = ''\n",
    "# for i in range(232):\n",
    "# #     pg_obj = reader.getPage(i)\n",
    "#     text = text + pg_obj.extractText()\n",
    "# #     print(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1CXtCenIbnsW"
   },
   "source": [
    "## PreProcessng Text File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2P805g6CbnsY"
   },
   "outputs": [],
   "source": [
    "pdf_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8FjEXX2jbnsf"
   },
   "outputs": [],
   "source": [
    "f = open(\"./One_Indian_Girl_-_Chetan_Bhagat-Redicals.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dLWRoaP8bnsk"
   },
   "outputs": [],
   "source": [
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o1zE7i-dbnss"
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "wPEm6nqCbns3",
    "outputId": "b208e88d-7611-423f-edbe-87d374b413d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464927\n",
      " \n",
      "\n",
      "ONE\n",
      "INDIAN\n",
      "GIRL\n",
      "\n",
      "Chetan Bhagat is the author of six bestselling novels—Fl've Point Someone (2004), One Night @ the\n",
      "Call Center (2005), The 3Mislakes ofMy Life (2008), 2 States (2009), Revolution 2020 (2011) and\n",
      "HalfGirlfriend (2014)—which have sold over ten million copies and have been translated into over\n",
      "a dozen languages worldwide.\n",
      "\n",
      "In 2008, The New York Times called him ‘the biggest—selling author in India’s history’, a\n",
      "position he has maintained to date. Almost all his books have been ad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:500])\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EV-fUl10bns-"
   },
   "outputs": [],
   "source": [
    "# text = text.lower()\n",
    "# text = re.sub(\"[^A-Za-z]+\",\" \",text)#^[a-zA-Z] means any a-z or A-Z at the start of a line\n",
    "# text = text.split()   #[^a-zA-Z] means any character that IS NOT a-z OR A-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5UN_lpfVbntE"
   },
   "source": [
    "## Normalise Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "YCrevEDpbntG",
    "outputId": "0cbfa940-d5dc-486c-bb7c-0aa538206dae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "11245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'“Hi, Brij esh.'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "tknizer = nltk.WordPunctTokenizer()\n",
    "stop_wrd = stopwords.words(\"english\")\n",
    "corpus = nltk.sent_tokenize(text)\n",
    "print(len(corpus))\n",
    "corpus[890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "44ONTTAfbntM"
   },
   "outputs": [],
   "source": [
    "def normalise_document(doc):\n",
    "    doc = re.sub(r\"^a-zA-Z\\s\",\"\",doc,re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    words = tknizer.tokenize(doc)\n",
    "#     print(words[5:10])\n",
    "    fil_words = [w for w in words if w not in stop_wrd]\n",
    "    \n",
    "    doc = \" \".join(fil_words)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRNfF017bntS"
   },
   "outputs": [],
   "source": [
    "norm_doc = np.vectorize(normalise_document) #it only transfer your functino in numpy for broad casting purpose\n",
    "doc  = norm_doc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Zv9BaNMebntY",
    "outputId": "d0b2b06c-7d8c-405e-f263-35aaad9a1d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "10299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'phone buzzed .'"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [sent for sent in doc if len(sent.split())>2]\n",
    "print(type(doc))\n",
    "print(len(doc))\n",
    "doc[890]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MEMpWAG5bntf"
   },
   "source": [
    "## Corpus Vocubulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PgZxJn7vbnth",
    "outputId": "3881df66-8fc1-43ab-8f09-fe3e0e12260d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6TDVgK6bbntr"
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(doc)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SqjyoxX6bnt-",
    "outputId": "615e4ccf-8435-45f5-bf81-ceb05c3d7901"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "hXFt9XHnbnuC",
    "outputId": "ab6ad335-7bfd-4a5d-f4b8-30c2429062c4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set',\n",
       " 'routines',\n",
       " 'protocols',\n",
       " 'tools',\n",
       " 'building',\n",
       " 'software',\n",
       " 'applications']"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text_to_word_sequence(doc[520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "UYZ0wLaxbnuH",
    "outputId": "159528dd-6070-487f-f32f-228be1f00d64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n",
      "('like', 6)\n",
      "(6, 'like')\n",
      "[1028, 80, 478, 1850, 368, 2391, 1851, 350, 858, 1150, 1852, 3491, 142, 602, 2392, 23, 2387]\n"
     ]
    }
   ],
   "source": [
    "word2id[\"pad\"] = 0\n",
    "id2word = dict([(v,k) for k , v in word2id.items()])\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(d)] for d in doc]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "emb_size = 100\n",
    "win_size = 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(list(word2id.items())[5])\n",
    "print(list(id2word.items())[5])\n",
    "print(list(wids)[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wuJk1aP7bnuP"
   },
   "source": [
    "## generate context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ttJWoBefbnuR"
   },
   "outputs": [],
   "source": [
    "def generate_context_pair(corpus , wind_size , vocab_size ):\n",
    "    for words in corpus:\n",
    "        for idx , word in enumerate(words):\n",
    "            cnt_word = []\n",
    "            label_word = []\n",
    "            start = idx - wind_size\n",
    "            end = idx +wind_size + 1\n",
    "            \n",
    "            cnt_word.append([words[i] for i in range(start,end) if i!= idx if 0 <= i < len(words)])\n",
    "            \n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(cnt_word,maxlen = wind_size * 2)\n",
    "            y = np_utils.to_categorical(label_word,vocab_size)\n",
    "            yield(x , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "JZCT49FgbnuY",
    "outputId": "e6729dcd-4758-435e-eef2-93dfc4c3badd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[60]]  <=>  [60]  <=>  60\n",
      "['one', 'indian', 'chetan', 'bhagat'] --> girl\n",
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[1028]]  <=>  [1028]  <=>  1028\n",
      "['indian', 'girl', 'bhagat', 'author'] --> chetan\n",
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[945]]  <=>  [945]  <=>  945\n",
      "['girl', 'chetan', 'author', 'six'] --> bhagat\n"
     ]
    }
   ],
   "source": [
    "# x = generate_context_pair(wids,2,vocab_size=vocab_size)\n",
    "cnt = 0\n",
    "for x ,y in generate_context_pair(wids,2,vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print(\"*\"*58)\n",
    "        print(\"shape of training data --> \",x.shape, y.shape)\n",
    "#         print(i[0]\n",
    "        \n",
    "        id_ = np.argwhere(y[0])\n",
    "        print(\"Return the idx of non zero value -\",id_,\" <=> \",id_[0],\" <=> \",id_[0][0])\n",
    "        print([id2word[id] for id in x[0]],\"-->\",id2word[id_[0][0]])\n",
    "    cnt+=1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eHLWyCk2bnud"
   },
   "source": [
    "## np.sum vs k.sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bE3fM15Wbnue"
   },
   "outputs": [],
   "source": [
    "def compareK_np_sum(x):\n",
    "        print()\n",
    "        print(\"^\"*60)\n",
    "        print(\"X --> \",x)\n",
    "        print(\"sum along axis 0 (np) -->\",np.sum(x,axis = 0,keepdims = True))\n",
    "        sum_ = K.sum(x,axis = 0 ,keepdims = True)\n",
    "        print(sum_)\n",
    "        print(\"sum along axis 1 (np) -->\",np.sum(x,axis = 1,keepdims = True))\n",
    "        sum__= K.sum(x,axis = 1 ,keepdims = True)\n",
    "        print(sum__)\n",
    "        print(\"^\"*60)\n",
    "        del sum_ ,sum__\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "03N29bMbbnum",
    "outputId": "42d90c42-d607-490b-c066-2b2106b4f11f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "X -->  [[2 3]\n",
      " [4 3]]\n",
      "sum along axis 0 (np) --> [[6 6]]\n",
      "Tensor(\"Sum:0\", shape=(1, 2), dtype=int64)\n",
      "sum along axis 1 (np) --> [[5]\n",
      " [7]]\n",
      "Tensor(\"Sum_1:0\", shape=(2, 1), dtype=int64)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_ = np.array([[2,3],[4,3]])\n",
    "print(x_.shape)\n",
    "compareK_np_sum(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCyqXOltbnuq"
   },
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bh-WYi1ebnuv"
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Dense , Embedding , Lambda\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "colab_type": "code",
    "id": "S8wtpIo3bnuz",
    "outputId": "d001a6f2-1764-4c2d-d78f-7e92bf1dc2a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0805 15:43:02.174518 139752826812288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0805 15:43:02.179760 139752826812288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0805 15:43:02.184044 139752826812288 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 100)            655600    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6556)              662156    \n",
      "=================================================================\n",
      "Total params: 1,317,756\n",
      "Trainable params: 1,317,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "\n",
    "cbow.add(Embedding(input_dim = vocab_size,output_dim = emb_size ,input_length = win_size * 2))\n",
    "cbow.add(Lambda(lambda x : K.mean(x , axis = 1)))\n",
    "cbow.add(Dense(vocab_size,activation = \"softmax\"))\n",
    "\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQXOEa_rbnu4"
   },
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OxCmJpwlbnu7"
   },
   "outputs": [],
   "source": [
    "cbow.compile(loss = \"categorical_crossentropy\",optimizer = \"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "4_TGg4GEbnvC",
    "outputId": "9bac8f6e-84c5-4809-e85f-8b13c9cb37f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \tloss 63401.61876747315\n",
      "\n",
      "epoch 2 \tloss 62374.24032921763\n",
      "\n",
      "epoch 3 \tloss 62288.088334573025\n",
      "\n",
      "epoch 4 \tloss 62185.4817619813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,5):\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x,y in generate_context_pair(wids,win_size,vocab_size):\n",
    "        cnt+=1\n",
    "#         print(loss)\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "        if cnt > vocab_size:\n",
    "           break\n",
    "        \n",
    "    print(\"epoch\",epoch , \"\\tloss\",loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MKI62rybeMn3"
   },
   "source": [
    "## Get Word **weights**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "MemGOWu-bnvG",
    "outputId": "c9e380e3-bd96-4efb-ae9d-2becde2d5459"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "      <th>32</th>\n",
       "      <th>33</th>\n",
       "      <th>34</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>...</th>\n",
       "      <th>60</th>\n",
       "      <th>61</th>\n",
       "      <th>62</th>\n",
       "      <th>63</th>\n",
       "      <th>64</th>\n",
       "      <th>65</th>\n",
       "      <th>66</th>\n",
       "      <th>67</th>\n",
       "      <th>68</th>\n",
       "      <th>69</th>\n",
       "      <th>70</th>\n",
       "      <th>71</th>\n",
       "      <th>72</th>\n",
       "      <th>73</th>\n",
       "      <th>74</th>\n",
       "      <th>75</th>\n",
       "      <th>76</th>\n",
       "      <th>77</th>\n",
       "      <th>78</th>\n",
       "      <th>79</th>\n",
       "      <th>80</th>\n",
       "      <th>81</th>\n",
       "      <th>82</th>\n",
       "      <th>83</th>\n",
       "      <th>84</th>\n",
       "      <th>85</th>\n",
       "      <th>86</th>\n",
       "      <th>87</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>-0.022720</td>\n",
       "      <td>0.135306</td>\n",
       "      <td>-0.053282</td>\n",
       "      <td>0.121484</td>\n",
       "      <td>0.598269</td>\n",
       "      <td>-0.056987</td>\n",
       "      <td>0.135193</td>\n",
       "      <td>0.099447</td>\n",
       "      <td>0.057409</td>\n",
       "      <td>-0.056849</td>\n",
       "      <td>0.090632</td>\n",
       "      <td>0.631652</td>\n",
       "      <td>0.156660</td>\n",
       "      <td>-0.098575</td>\n",
       "      <td>0.101455</td>\n",
       "      <td>0.136497</td>\n",
       "      <td>0.350908</td>\n",
       "      <td>-0.343832</td>\n",
       "      <td>-0.361281</td>\n",
       "      <td>-0.286316</td>\n",
       "      <td>0.467748</td>\n",
       "      <td>0.072726</td>\n",
       "      <td>-0.249610</td>\n",
       "      <td>0.344174</td>\n",
       "      <td>-0.070469</td>\n",
       "      <td>-0.164689</td>\n",
       "      <td>0.223758</td>\n",
       "      <td>0.209016</td>\n",
       "      <td>-0.149447</td>\n",
       "      <td>0.061665</td>\n",
       "      <td>-0.026548</td>\n",
       "      <td>-0.146506</td>\n",
       "      <td>-0.395806</td>\n",
       "      <td>0.540297</td>\n",
       "      <td>-0.484221</td>\n",
       "      <td>-0.011347</td>\n",
       "      <td>0.140885</td>\n",
       "      <td>0.241479</td>\n",
       "      <td>0.160085</td>\n",
       "      <td>0.046961</td>\n",
       "      <td>...</td>\n",
       "      <td>0.688237</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>0.115009</td>\n",
       "      <td>0.820390</td>\n",
       "      <td>-0.261331</td>\n",
       "      <td>0.343398</td>\n",
       "      <td>-0.112369</td>\n",
       "      <td>-0.178766</td>\n",
       "      <td>0.230802</td>\n",
       "      <td>-0.004720</td>\n",
       "      <td>-0.234477</td>\n",
       "      <td>-0.279297</td>\n",
       "      <td>0.025190</td>\n",
       "      <td>-0.256675</td>\n",
       "      <td>0.070261</td>\n",
       "      <td>0.291632</td>\n",
       "      <td>-0.576976</td>\n",
       "      <td>-0.037627</td>\n",
       "      <td>0.116572</td>\n",
       "      <td>-0.460719</td>\n",
       "      <td>0.110156</td>\n",
       "      <td>-0.084440</td>\n",
       "      <td>0.047169</td>\n",
       "      <td>0.294285</td>\n",
       "      <td>0.075878</td>\n",
       "      <td>0.209045</td>\n",
       "      <td>0.023149</td>\n",
       "      <td>-0.018986</td>\n",
       "      <td>-0.316334</td>\n",
       "      <td>-0.201368</td>\n",
       "      <td>0.119469</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>-0.232622</td>\n",
       "      <td>0.127752</td>\n",
       "      <td>-0.153808</td>\n",
       "      <td>-0.166507</td>\n",
       "      <td>-0.200365</td>\n",
       "      <td>-0.286396</td>\n",
       "      <td>-0.145619</td>\n",
       "      <td>0.104314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>-0.071047</td>\n",
       "      <td>-0.216564</td>\n",
       "      <td>-0.271113</td>\n",
       "      <td>-0.234575</td>\n",
       "      <td>0.478464</td>\n",
       "      <td>-0.514455</td>\n",
       "      <td>0.305571</td>\n",
       "      <td>0.184223</td>\n",
       "      <td>-0.013023</td>\n",
       "      <td>-0.163950</td>\n",
       "      <td>-0.081322</td>\n",
       "      <td>0.082421</td>\n",
       "      <td>0.345518</td>\n",
       "      <td>0.331016</td>\n",
       "      <td>0.221741</td>\n",
       "      <td>-0.131270</td>\n",
       "      <td>0.256285</td>\n",
       "      <td>-0.294370</td>\n",
       "      <td>-0.205948</td>\n",
       "      <td>-0.256354</td>\n",
       "      <td>0.244627</td>\n",
       "      <td>0.019041</td>\n",
       "      <td>-0.554600</td>\n",
       "      <td>0.064867</td>\n",
       "      <td>0.480522</td>\n",
       "      <td>0.663244</td>\n",
       "      <td>0.594983</td>\n",
       "      <td>-0.398030</td>\n",
       "      <td>-0.130733</td>\n",
       "      <td>-0.158510</td>\n",
       "      <td>-0.038736</td>\n",
       "      <td>0.242217</td>\n",
       "      <td>-0.184277</td>\n",
       "      <td>-0.194513</td>\n",
       "      <td>0.376491</td>\n",
       "      <td>-0.137999</td>\n",
       "      <td>-0.087950</td>\n",
       "      <td>-0.029856</td>\n",
       "      <td>-0.052758</td>\n",
       "      <td>0.957080</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.019394</td>\n",
       "      <td>-0.296147</td>\n",
       "      <td>0.113076</td>\n",
       "      <td>0.122716</td>\n",
       "      <td>-0.292727</td>\n",
       "      <td>-0.431050</td>\n",
       "      <td>-0.333651</td>\n",
       "      <td>0.268257</td>\n",
       "      <td>-0.096615</td>\n",
       "      <td>0.338092</td>\n",
       "      <td>0.177870</td>\n",
       "      <td>0.091716</td>\n",
       "      <td>0.150719</td>\n",
       "      <td>0.235350</td>\n",
       "      <td>-0.237254</td>\n",
       "      <td>0.608982</td>\n",
       "      <td>0.064278</td>\n",
       "      <td>0.154034</td>\n",
       "      <td>0.052718</td>\n",
       "      <td>0.163398</td>\n",
       "      <td>0.431844</td>\n",
       "      <td>0.407980</td>\n",
       "      <td>-0.172432</td>\n",
       "      <td>0.107723</td>\n",
       "      <td>-0.380203</td>\n",
       "      <td>-0.228786</td>\n",
       "      <td>-0.041154</td>\n",
       "      <td>-0.134404</td>\n",
       "      <td>0.001685</td>\n",
       "      <td>0.113732</td>\n",
       "      <td>-0.246785</td>\n",
       "      <td>-0.325208</td>\n",
       "      <td>-0.185590</td>\n",
       "      <td>0.284324</td>\n",
       "      <td>0.212001</td>\n",
       "      <td>-0.247207</td>\n",
       "      <td>0.209432</td>\n",
       "      <td>-0.224090</td>\n",
       "      <td>-0.368581</td>\n",
       "      <td>0.556418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neel</th>\n",
       "      <td>-0.254439</td>\n",
       "      <td>-0.317121</td>\n",
       "      <td>-0.025290</td>\n",
       "      <td>-0.351523</td>\n",
       "      <td>0.606926</td>\n",
       "      <td>0.257070</td>\n",
       "      <td>0.744285</td>\n",
       "      <td>-0.649370</td>\n",
       "      <td>0.328571</td>\n",
       "      <td>-0.688320</td>\n",
       "      <td>-0.298092</td>\n",
       "      <td>-0.182848</td>\n",
       "      <td>0.338710</td>\n",
       "      <td>-0.525844</td>\n",
       "      <td>-0.503951</td>\n",
       "      <td>0.530372</td>\n",
       "      <td>0.422124</td>\n",
       "      <td>0.890085</td>\n",
       "      <td>0.115014</td>\n",
       "      <td>-0.340959</td>\n",
       "      <td>-0.242414</td>\n",
       "      <td>0.930674</td>\n",
       "      <td>-0.265488</td>\n",
       "      <td>0.054730</td>\n",
       "      <td>-0.271078</td>\n",
       "      <td>-0.401899</td>\n",
       "      <td>-0.256024</td>\n",
       "      <td>-0.149805</td>\n",
       "      <td>0.311821</td>\n",
       "      <td>0.091444</td>\n",
       "      <td>-0.599095</td>\n",
       "      <td>-0.129380</td>\n",
       "      <td>-0.106493</td>\n",
       "      <td>-0.274409</td>\n",
       "      <td>0.929977</td>\n",
       "      <td>0.194982</td>\n",
       "      <td>0.232767</td>\n",
       "      <td>0.065394</td>\n",
       "      <td>0.766891</td>\n",
       "      <td>0.115428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046601</td>\n",
       "      <td>-0.461798</td>\n",
       "      <td>0.016357</td>\n",
       "      <td>0.556118</td>\n",
       "      <td>0.291537</td>\n",
       "      <td>-0.650683</td>\n",
       "      <td>-0.128858</td>\n",
       "      <td>0.103960</td>\n",
       "      <td>-0.195352</td>\n",
       "      <td>-0.900191</td>\n",
       "      <td>0.038515</td>\n",
       "      <td>-0.162373</td>\n",
       "      <td>0.144938</td>\n",
       "      <td>0.506582</td>\n",
       "      <td>0.873350</td>\n",
       "      <td>-0.423681</td>\n",
       "      <td>-0.076523</td>\n",
       "      <td>0.347845</td>\n",
       "      <td>-0.001649</td>\n",
       "      <td>0.086025</td>\n",
       "      <td>-0.447499</td>\n",
       "      <td>0.236199</td>\n",
       "      <td>0.146426</td>\n",
       "      <td>0.151752</td>\n",
       "      <td>0.944804</td>\n",
       "      <td>-0.138569</td>\n",
       "      <td>0.108387</td>\n",
       "      <td>0.435776</td>\n",
       "      <td>0.016523</td>\n",
       "      <td>0.056811</td>\n",
       "      <td>0.072710</td>\n",
       "      <td>1.044848</td>\n",
       "      <td>0.143544</td>\n",
       "      <td>0.117475</td>\n",
       "      <td>-0.389716</td>\n",
       "      <td>-0.332104</td>\n",
       "      <td>-0.624271</td>\n",
       "      <td>0.273954</td>\n",
       "      <td>-0.283569</td>\n",
       "      <td>-0.725304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debu</th>\n",
       "      <td>0.079101</td>\n",
       "      <td>-0.191456</td>\n",
       "      <td>-0.075812</td>\n",
       "      <td>-0.109067</td>\n",
       "      <td>0.123679</td>\n",
       "      <td>0.139692</td>\n",
       "      <td>-0.046820</td>\n",
       "      <td>-0.206601</td>\n",
       "      <td>-0.167150</td>\n",
       "      <td>-0.116812</td>\n",
       "      <td>0.029807</td>\n",
       "      <td>-0.009269</td>\n",
       "      <td>0.218260</td>\n",
       "      <td>-0.221215</td>\n",
       "      <td>-0.043163</td>\n",
       "      <td>0.243244</td>\n",
       "      <td>0.088473</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>-0.028076</td>\n",
       "      <td>0.334555</td>\n",
       "      <td>-0.025995</td>\n",
       "      <td>0.192249</td>\n",
       "      <td>-0.202985</td>\n",
       "      <td>-0.084837</td>\n",
       "      <td>0.138314</td>\n",
       "      <td>-0.029930</td>\n",
       "      <td>-0.292446</td>\n",
       "      <td>-0.024085</td>\n",
       "      <td>-0.145456</td>\n",
       "      <td>0.072794</td>\n",
       "      <td>0.060098</td>\n",
       "      <td>-0.111943</td>\n",
       "      <td>0.230706</td>\n",
       "      <td>-0.378851</td>\n",
       "      <td>-0.045952</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.115890</td>\n",
       "      <td>-0.024007</td>\n",
       "      <td>0.287755</td>\n",
       "      <td>0.051467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.268751</td>\n",
       "      <td>-0.040652</td>\n",
       "      <td>-0.025268</td>\n",
       "      <td>-0.038200</td>\n",
       "      <td>-0.055769</td>\n",
       "      <td>-0.010843</td>\n",
       "      <td>-0.039047</td>\n",
       "      <td>-0.114481</td>\n",
       "      <td>-0.056893</td>\n",
       "      <td>-0.354773</td>\n",
       "      <td>-0.111986</td>\n",
       "      <td>-0.015663</td>\n",
       "      <td>0.454939</td>\n",
       "      <td>0.046737</td>\n",
       "      <td>0.177847</td>\n",
       "      <td>0.138950</td>\n",
       "      <td>0.239357</td>\n",
       "      <td>0.075996</td>\n",
       "      <td>0.063850</td>\n",
       "      <td>0.425224</td>\n",
       "      <td>-0.076774</td>\n",
       "      <td>0.081448</td>\n",
       "      <td>0.121748</td>\n",
       "      <td>-0.071379</td>\n",
       "      <td>0.157690</td>\n",
       "      <td>-0.013687</td>\n",
       "      <td>-0.047884</td>\n",
       "      <td>-0.111301</td>\n",
       "      <td>-0.024795</td>\n",
       "      <td>-0.102052</td>\n",
       "      <td>0.175763</td>\n",
       "      <td>-0.130294</td>\n",
       "      <td>0.032943</td>\n",
       "      <td>0.051100</td>\n",
       "      <td>-0.163464</td>\n",
       "      <td>0.172571</td>\n",
       "      <td>-0.175847</td>\n",
       "      <td>0.276834</td>\n",
       "      <td>0.033702</td>\n",
       "      <td>-0.180989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.024025</td>\n",
       "      <td>-0.111846</td>\n",
       "      <td>-0.180834</td>\n",
       "      <td>-0.190905</td>\n",
       "      <td>0.096434</td>\n",
       "      <td>0.004204</td>\n",
       "      <td>-0.154967</td>\n",
       "      <td>-0.233875</td>\n",
       "      <td>-0.082517</td>\n",
       "      <td>-0.063315</td>\n",
       "      <td>0.028798</td>\n",
       "      <td>0.086468</td>\n",
       "      <td>0.156173</td>\n",
       "      <td>-0.258505</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.285562</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>0.143057</td>\n",
       "      <td>-0.116560</td>\n",
       "      <td>0.242926</td>\n",
       "      <td>0.037774</td>\n",
       "      <td>0.278875</td>\n",
       "      <td>-0.188272</td>\n",
       "      <td>-0.079485</td>\n",
       "      <td>0.250280</td>\n",
       "      <td>-0.016250</td>\n",
       "      <td>-0.001199</td>\n",
       "      <td>-0.172533</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>0.129417</td>\n",
       "      <td>-0.041981</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>-0.227190</td>\n",
       "      <td>-0.159310</td>\n",
       "      <td>-0.118246</td>\n",
       "      <td>0.115272</td>\n",
       "      <td>-0.000028</td>\n",
       "      <td>-0.064672</td>\n",
       "      <td>0.214774</td>\n",
       "      <td>-0.017278</td>\n",
       "      <td>...</td>\n",
       "      <td>0.210811</td>\n",
       "      <td>-0.191131</td>\n",
       "      <td>0.110307</td>\n",
       "      <td>0.106230</td>\n",
       "      <td>-0.006660</td>\n",
       "      <td>-0.188051</td>\n",
       "      <td>0.017953</td>\n",
       "      <td>-0.096126</td>\n",
       "      <td>0.010511</td>\n",
       "      <td>-0.325529</td>\n",
       "      <td>0.025202</td>\n",
       "      <td>-0.065541</td>\n",
       "      <td>-0.025933</td>\n",
       "      <td>0.163005</td>\n",
       "      <td>0.342708</td>\n",
       "      <td>0.092819</td>\n",
       "      <td>-0.039346</td>\n",
       "      <td>0.046283</td>\n",
       "      <td>0.103579</td>\n",
       "      <td>0.028244</td>\n",
       "      <td>-0.107619</td>\n",
       "      <td>0.150068</td>\n",
       "      <td>-0.219627</td>\n",
       "      <td>0.059553</td>\n",
       "      <td>0.192227</td>\n",
       "      <td>-0.011362</td>\n",
       "      <td>-0.038693</td>\n",
       "      <td>-0.031027</td>\n",
       "      <td>-0.255753</td>\n",
       "      <td>-0.028828</td>\n",
       "      <td>0.019747</td>\n",
       "      <td>0.256152</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.123443</td>\n",
       "      <td>-0.258801</td>\n",
       "      <td>-0.107306</td>\n",
       "      <td>-0.216792</td>\n",
       "      <td>0.063142</td>\n",
       "      <td>-0.198352</td>\n",
       "      <td>-0.179109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.084937</td>\n",
       "      <td>-0.240218</td>\n",
       "      <td>-0.072935</td>\n",
       "      <td>-0.038096</td>\n",
       "      <td>-0.105243</td>\n",
       "      <td>-0.175551</td>\n",
       "      <td>0.065739</td>\n",
       "      <td>-0.228755</td>\n",
       "      <td>-0.063310</td>\n",
       "      <td>0.173444</td>\n",
       "      <td>-0.098846</td>\n",
       "      <td>0.026524</td>\n",
       "      <td>0.307374</td>\n",
       "      <td>-0.089696</td>\n",
       "      <td>0.019679</td>\n",
       "      <td>0.138053</td>\n",
       "      <td>0.107860</td>\n",
       "      <td>0.135175</td>\n",
       "      <td>-0.051900</td>\n",
       "      <td>-0.046255</td>\n",
       "      <td>0.106559</td>\n",
       "      <td>0.099577</td>\n",
       "      <td>-0.008820</td>\n",
       "      <td>-0.178309</td>\n",
       "      <td>0.034403</td>\n",
       "      <td>-0.121889</td>\n",
       "      <td>0.056801</td>\n",
       "      <td>-0.131504</td>\n",
       "      <td>-0.181001</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.024224</td>\n",
       "      <td>0.191863</td>\n",
       "      <td>0.008406</td>\n",
       "      <td>0.044727</td>\n",
       "      <td>-0.051487</td>\n",
       "      <td>0.100010</td>\n",
       "      <td>0.146506</td>\n",
       "      <td>-0.082078</td>\n",
       "      <td>0.135525</td>\n",
       "      <td>0.165548</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011767</td>\n",
       "      <td>-0.134594</td>\n",
       "      <td>0.163293</td>\n",
       "      <td>-0.108635</td>\n",
       "      <td>-0.001633</td>\n",
       "      <td>0.092515</td>\n",
       "      <td>-0.110011</td>\n",
       "      <td>-0.197170</td>\n",
       "      <td>0.062268</td>\n",
       "      <td>-0.175438</td>\n",
       "      <td>-0.024516</td>\n",
       "      <td>-0.143082</td>\n",
       "      <td>-0.149140</td>\n",
       "      <td>0.291062</td>\n",
       "      <td>0.137775</td>\n",
       "      <td>0.023477</td>\n",
       "      <td>-0.033106</td>\n",
       "      <td>0.139248</td>\n",
       "      <td>-0.070840</td>\n",
       "      <td>-0.101666</td>\n",
       "      <td>0.195316</td>\n",
       "      <td>0.241559</td>\n",
       "      <td>-0.181208</td>\n",
       "      <td>0.119944</td>\n",
       "      <td>0.094083</td>\n",
       "      <td>-0.116352</td>\n",
       "      <td>0.194638</td>\n",
       "      <td>0.030440</td>\n",
       "      <td>-0.251851</td>\n",
       "      <td>-0.012721</td>\n",
       "      <td>-0.130297</td>\n",
       "      <td>0.066632</td>\n",
       "      <td>-0.059250</td>\n",
       "      <td>0.091856</td>\n",
       "      <td>-0.051275</td>\n",
       "      <td>0.009490</td>\n",
       "      <td>-0.085670</td>\n",
       "      <td>-0.157734</td>\n",
       "      <td>-0.245016</td>\n",
       "      <td>-0.020597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2   ...        97        98        99\n",
       "‘    -0.022720  0.135306 -0.053282  ... -0.286396 -0.145619  0.104314\n",
       "said -0.071047 -0.216564 -0.271113  ... -0.224090 -0.368581  0.556418\n",
       "neel -0.254439 -0.317121 -0.025290  ...  0.273954 -0.283569 -0.725304\n",
       "debu  0.079101 -0.191456 -0.075812  ...  0.276834  0.033702 -0.180989\n",
       "like  0.024025 -0.111846 -0.180834  ...  0.063142 -0.198352 -0.179109\n",
       "one   0.084937 -0.240218 -0.072935  ... -0.157734 -0.245016 -0.020597\n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "\n",
    "print(weights.shape)\n",
    "\n",
    "import pandas as pd\n",
    "wrd_emb = pd.DataFrame(weights , index = list(id2word.values())[1:])\n",
    "wrd_emb.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1oDNaySe3P7"
   },
   "source": [
    "## Simililarity of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "d_zAV8R5eceU",
    "outputId": "bfaaa37c-010e-45c4-8a03-34751c9e26d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555, 6555)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'airport': ['14', 'shudder', 'writer', 'logi', 'goof'],\n",
       " 'boys': ['fl', 'goof', 'gary', 'negotiate', 'torch'],\n",
       " 'bus': ['fairy', 'disruption', 'ﬁimbling', 'hindsight', 'americans'],\n",
       " 'california': ['lcthl', 'singles', 'gents', 'split', 'l'],\n",
       " 'chetan': ['jiju', 'gary', 'weds', 'decreased', 'advertisement'],\n",
       " 'facebook': ['feared', 'peg', 'goof', 'gary', 'vivita'],\n",
       " 'man': ['consume', 'fl', 'torch', 'temptation', 'exaggerating'],\n",
       " 'phone': ['torch', 'bearer', 'heaved', 'various', 'prizes']}"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "c_matrix = cosine_similarity(weights)\n",
    "print(c_matrix.shape)\n",
    "\n",
    "similar_words = {search_term: [id2word[idx] for idx in c_matrix[word2id[search_term]-1].argsort()[1:6]+1] \n",
    "                   for search_term in ['facebook', 'california', 'phone', 'bus', 'chetan', 'airport', 'boys','man']}\n",
    "\n",
    "similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXxSFHSye6LI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "CBOW_Word_Emb_Model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
