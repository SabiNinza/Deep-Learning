{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import PyPDF2\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readtxt(path):\n",
    "    f = open(path)\n",
    "    text = f.read()\n",
    "    f.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = readtxt(\"./One_Indian_Girl_-_Chetan_Bhagat-Redicals.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464927\n",
      " \n",
      "\n",
      "ONE\n",
      "INDIAN\n",
      "GIRL\n",
      "\n",
      "Chetan Bhagat is the author of six bestselling novels—Fl've Point Someone (2004), One Night @ the\n",
      "Call Center (2005), The 3Mislakes ofMy Life (2008), 2 States (2009), Revolution 2020 (2011) and\n",
      "HalfGirlfriend (2014)—which have sold over ten million copies and have been translated into over\n",
      "a dozen languages worldwide.\n",
      "\n",
      "In 2008, The New York Times called him ‘the biggest—selling author in India’s history’, a\n",
      "position he has maintained to date. Almost all his books have been ad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:500])\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/puneet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/puneet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'“Hi, Brij esh.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt\")\n",
    "tknizer = nltk.WordPunctTokenizer()\n",
    "stop_wrd = stopwords.words(\"english\")\n",
    "corpus = nltk.sent_tokenize(text)\n",
    "print(len(corpus))\n",
    "corpus[890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_document(doc):\n",
    "    doc = re.sub(r\"^a-zA-Z\\s\",\"\",doc,re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    words = tknizer.tokenize(doc)\n",
    "#     print(words[5:10])\n",
    "    fil_words = [w for w in words if w not in stop_wrd]\n",
    "    \n",
    "    doc = \" \".join(fil_words)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_doc = np.vectorize(normalise_document) #it only transfer your functino in numpy for broad casting purpose\n",
    "doc  = norm_doc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "10299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'phone buzzed .'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [sent for sent in doc if len(sent.split())>2]\n",
    "print(type(doc))\n",
    "print(len(doc))\n",
    "doc[890]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Vocubalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/puneet/.local/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['set',\n",
       " 'routines',\n",
       " 'protocols',\n",
       " 'tools',\n",
       " 'building',\n",
       " 'software',\n",
       " 'applications']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(doc)\n",
    "word2id = tokenizer.word_index\n",
    "print(len(word2id.keys()))\n",
    "text.text_to_word_sequence(doc[520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n",
      "('like', 6)\n",
      "(6, 'like')\n",
      "[1028, 80, 478, 1850, 368, 2391, 1851, 350, 858, 1150, 1852, 3491, 142, 602, 2392, 23, 2387]\n"
     ]
    }
   ],
   "source": [
    "word2id[\"pad\"] = 0\n",
    "id2word = dict([(v,k) for k , v in word2id.items()])\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(d)] for d in doc]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "emb_size = 100\n",
    "win_size = 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(list(word2id.items())[5])\n",
    "print(list(id2word.items())[5])\n",
    "print(list(wids)[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip-Gram Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import skipgrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2 (733), iauren (6252)) -> 0\n",
      "(someone (128), point (274)) -> 1\n",
      "(call (45), 2005 (3462)) -> 1\n",
      "(worldwide (1847), career (335)) -> 0\n",
      "(' (1523), matching (1672)) -> 0\n",
      "(six (322), one (7)) -> 1\n",
      "(point (274), 21 (5202)) -> 0\n",
      "(ofmy (1306), revolution (3466)) -> 1\n",
      "(2011 (3468), iet (307)) -> 0\n",
      "(2011 (3468), hovering (2172)) -> 0\n"
     ]
    }
   ],
   "source": [
    "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=10) for wid in wids]\n",
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(10):\n",
    "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\n",
    "          id2word[pairs[i][0]], pairs[i][0], \n",
    "          id2word[pairs[i][1]], pairs[i][1], \n",
    "          labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense , Reshape ,Embedding\n",
    "from keras.layers import dot\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = Sequential()\n",
    "word_model.add(Embedding(vocab_size,emb_size,input_length=1))\n",
    "word_model.add(Reshape((emb_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_model =Sequential()\n",
    "context_model.add(Embedding(vocab_size,emb_size,input_length=1))\n",
    "context_model.add(Reshape((emb_size,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# model.add(dot([word_model,context_model],axes=))\n",
    "model.add(Dense(1,activation=\"sigmoid\"))\n",
    "model.compile(loss = \"mean_squared_error\",optimizer = \"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
