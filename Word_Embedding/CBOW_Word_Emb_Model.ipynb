{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PyPDF2\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# !pip3 install textract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pdf extact text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_obj = open(\"One_Indian_Girl_-_Chetan_Bhagat-Redicals.pdf\",\"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PyPDF2.PdfFileReader(pdf_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "No_of_pg = reader.numPages\n",
    "print(No_of_pg)\n",
    "pg_obj = reader.getPage(50)\n",
    "pg_obj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ''\n",
    "# for i in range(232):\n",
    "# #     pg_obj = reader.getPage(i)\n",
    "#     text = text + pg_obj.extractText()\n",
    "# #     print(text)\n",
    "# text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PreProcessng Text File "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"./One_Indian_Girl_-_Chetan_Bhagat-Redicals.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464927\n",
      " \n",
      "\n",
      "ONE\n",
      "INDIAN\n",
      "GIRL\n",
      "\n",
      "Chetan Bhagat is the author of six bestselling novels—Fl've Point Someone (2004), One Night @ the\n",
      "Call Center (2005), The 3Mislakes ofMy Life (2008), 2 States (2009), Revolution 2020 (2011) and\n",
      "HalfGirlfriend (2014)—which have sold over ten million copies and have been translated into over\n",
      "a dozen languages worldwide.\n",
      "\n",
      "In 2008, The New York Times called him ‘the biggest—selling author in India’s history’, a\n",
      "position he has maintained to date. Almost all his books have been ad\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(text[:500])\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = text.lower()\n",
    "# text = re.sub(\"[^A-Za-z]+\",\" \",text)#^[a-zA-Z] means any a-z or A-Z at the start of a line\n",
    "# text = text.split()   #[^a-zA-Z] means any character that IS NOT a-z OR A-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalise Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11245\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'“Hi, Brij esh.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tknizer = nltk.WordPunctTokenizer()\n",
    "stop_wrd = stopwords.words(\"english\")\n",
    "corpus = nltk.sent_tokenize(text)\n",
    "print(len(corpus))\n",
    "corpus[890]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_document(doc):\n",
    "    doc = re.sub(r\"^a-zA-Z\\s\",\"\",doc,re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    words = tknizer.tokenize(doc)\n",
    "#     print(words[5:10])\n",
    "    fil_words = [w for w in words if w not in stop_wrd]\n",
    "    \n",
    "    doc = \" \".join(fil_words)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_doc = np.vectorize(normalise_document) #it only transfer your functino in numpy for broad casting purpose\n",
    "doc  = norm_doc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "10299\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'phone buzzed .'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = [sent for sent in doc if len(sent.split())>2]\n",
    "print(type(doc))\n",
    "print(len(doc))\n",
    "doc[890]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Vocubulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "RequestsDependencyWarning: urllib3 (1.24.1) or chardet (2.3.0) doesn't match a supported version! [__init__.py:91]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import text\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer()\n",
    "tokenizer.fit_on_texts(doc)\n",
    "word2id = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n"
     ]
    }
   ],
   "source": [
    "print(len(word2id.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['set',\n",
       " 'routines',\n",
       " 'protocols',\n",
       " 'tools',\n",
       " 'building',\n",
       " 'software',\n",
       " 'applications']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.text_to_word_sequence(doc[520])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6556\n",
      "('like', 6)\n",
      "(6, 'like')\n",
      "[1028, 80, 478, 1850, 368, 2391, 1851, 350, 858, 1150, 1852, 3491, 142, 602, 2392, 23, 2387]\n"
     ]
    }
   ],
   "source": [
    "word2id[\"pad\"] = 0\n",
    "id2word = dict([(v,k) for k , v in word2id.items()])\n",
    "wids = [[word2id[w] for w in text.text_to_word_sequence(d)] for d in doc]\n",
    "\n",
    "vocab_size = len(word2id)\n",
    "emb_size = 100\n",
    "win_size = 1\n",
    "\n",
    "print(vocab_size)\n",
    "print(list(word2id.items())[5])\n",
    "print(list(id2word.items())[5])\n",
    "print(list(wids)[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate context word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_context_pair(corpus , wind_size , vocab_size ):\n",
    "    for words in corpus:\n",
    "        for idx , word in enumerate(words):\n",
    "            cnt_word = []\n",
    "            label_word = []\n",
    "            start = idx - wind_size\n",
    "            end = idx +wind_size + 1\n",
    "            \n",
    "            cnt_word.append([words[i] for i in range(start,end) if i!= idx if 0 <= i < len(words)])\n",
    "            \n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = sequence.pad_sequences(cnt_word,maxlen = wind_size * 2)\n",
    "            y = np_utils.to_categorical(label_word,vocab_size)\n",
    "            yield(x , y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[60]]  <=>  [60]  <=>  60\n",
      "['one', 'indian', 'chetan', 'bhagat'] --> girl\n",
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[1028]]  <=>  [1028]  <=>  1028\n",
      "['indian', 'girl', 'bhagat', 'author'] --> chetan\n",
      "**********************************************************\n",
      "shape of training data -->  (1, 4) (1, 6556)\n",
      "Return the idx of non zero value - [[945]]  <=>  [945]  <=>  945\n",
      "['girl', 'chetan', 'author', 'six'] --> bhagat\n"
     ]
    }
   ],
   "source": [
    "# x = generate_context_pair(wids,2,vocab_size=vocab_size)\n",
    "cnt = 0\n",
    "for x ,y in generate_context_pair(wids,2,vocab_size):\n",
    "    if 0 not in x[0]:\n",
    "        print(\"*\"*58)\n",
    "        print(\"shape of training data --> \",x.shape, y.shape)\n",
    "#         print(i[0]\n",
    "        \n",
    "        id_ = np.argwhere(y[0])\n",
    "        print(\"Return the idx of non zero value -\",id_,\" <=> \",id_[0],\" <=> \",id_[0][0])\n",
    "        print([id2word[id] for id in x[0]],\"-->\",id2word[id_[0][0]])\n",
    "    cnt+=1\n",
    "    if cnt == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## np.sum vs k.sum2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareK_np_sum(x):\n",
    "        print()\n",
    "        print(\"^\"*60)\n",
    "        print(\"X --> \",x)\n",
    "        print(\"sum along axis 0 (np) -->\",np.sum(x,axis = 0,keepdims = True))\n",
    "        sum_ = K.sum(x,axis = 0 ,keepdims = True)\n",
    "        print(sum_)\n",
    "        print(\"sum along axis 1 (np) -->\",np.sum(x,axis = 1,keepdims = True))\n",
    "        sum__= K.sum(x,axis = 1 ,keepdims = True)\n",
    "        print(sum__)\n",
    "        print(\"^\"*60)\n",
    "        del sum_ ,sum__\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n",
      "\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "X -->  [[2 3]\n",
      " [4 3]]\n",
      "sum along axis 0 (np) --> [[6 6]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'K' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e8b4274a5cef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcompareK_np_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-26-ad3e4e5b7127>\u001b[0m in \u001b[0;36mcompareK_np_sum\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X --> \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum along axis 0 (np) -->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0msum_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum along axis 1 (np) -->\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
     ]
    }
   ],
   "source": [
    "x_ = np.array([[2,3],[4,3]])\n",
    "print(x_.shape)\n",
    "compareK_np_sum(x_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Dense , Embedding , Lambda\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/puneet/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2, 100)            655600    \n",
      "_________________________________________________________________\n",
      "lambda_1 (Lambda)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6556)              662156    \n",
      "=================================================================\n",
      "Total params: 1,317,756\n",
      "Trainable params: 1,317,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential()\n",
    "\n",
    "cbow.add(Embedding(input_dim = vocab_size,output_dim = emb_size ,input_length = win_size * 2))\n",
    "cbow.add(Lambda(lambda x : K.mean(x , axis = 1)))\n",
    "cbow.add(Dense(vocab_size,activation = \"softmax\"))\n",
    "\n",
    "print(cbow.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.compile(loss = \"categorical_crossentropy\",optimizer = \"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 \tloss 45830.77659081883\n",
      "\n",
      "epoch 2 \tloss 46036.16768140702\n",
      "\n",
      "epoch 3 \tloss 46242.21440684785\n",
      "\n",
      "epoch 4 \tloss 46018.432844458104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,5):\n",
    "    loss = 0\n",
    "    cnt = 0\n",
    "    for x,y in generate_context_pair(wids,win_size,vocab_size):\n",
    "        cnt+=1\n",
    "#         print(loss)\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "        \n",
    "        if cnt > 5000:\n",
    "            break\n",
    "        \n",
    "    print(\"epoch\",epoch , \"\\tloss\",loss)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6555, 100)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>‘</th>\n",
       "      <td>0.281076</td>\n",
       "      <td>-0.181313</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>-0.122151</td>\n",
       "      <td>0.152994</td>\n",
       "      <td>-0.095516</td>\n",
       "      <td>-0.029143</td>\n",
       "      <td>0.377922</td>\n",
       "      <td>0.280936</td>\n",
       "      <td>-0.166578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027879</td>\n",
       "      <td>-0.259911</td>\n",
       "      <td>-0.394983</td>\n",
       "      <td>-0.462198</td>\n",
       "      <td>-0.138003</td>\n",
       "      <td>-0.036760</td>\n",
       "      <td>0.360835</td>\n",
       "      <td>0.145279</td>\n",
       "      <td>-0.347896</td>\n",
       "      <td>-0.358649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>said</th>\n",
       "      <td>0.119032</td>\n",
       "      <td>-0.598810</td>\n",
       "      <td>0.103481</td>\n",
       "      <td>-0.323416</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.091895</td>\n",
       "      <td>-0.107412</td>\n",
       "      <td>-0.134377</td>\n",
       "      <td>-0.170499</td>\n",
       "      <td>-0.045597</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.846326</td>\n",
       "      <td>-0.249982</td>\n",
       "      <td>0.531245</td>\n",
       "      <td>0.477507</td>\n",
       "      <td>-0.057289</td>\n",
       "      <td>0.094422</td>\n",
       "      <td>0.100349</td>\n",
       "      <td>-0.397421</td>\n",
       "      <td>-0.038629</td>\n",
       "      <td>-0.398164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neel</th>\n",
       "      <td>-0.237389</td>\n",
       "      <td>-0.141943</td>\n",
       "      <td>-0.566874</td>\n",
       "      <td>-0.403153</td>\n",
       "      <td>-0.492787</td>\n",
       "      <td>-0.356656</td>\n",
       "      <td>0.072839</td>\n",
       "      <td>-0.228308</td>\n",
       "      <td>-0.123777</td>\n",
       "      <td>-0.424938</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.072097</td>\n",
       "      <td>1.043128</td>\n",
       "      <td>-0.046427</td>\n",
       "      <td>0.276616</td>\n",
       "      <td>0.250141</td>\n",
       "      <td>0.135944</td>\n",
       "      <td>-0.296210</td>\n",
       "      <td>0.035597</td>\n",
       "      <td>-0.020644</td>\n",
       "      <td>-0.271495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debu</th>\n",
       "      <td>-0.265364</td>\n",
       "      <td>-0.161909</td>\n",
       "      <td>-0.280103</td>\n",
       "      <td>-0.210716</td>\n",
       "      <td>-0.160269</td>\n",
       "      <td>0.242691</td>\n",
       "      <td>-0.019847</td>\n",
       "      <td>0.310536</td>\n",
       "      <td>-0.126590</td>\n",
       "      <td>-0.137395</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.122794</td>\n",
       "      <td>-0.123167</td>\n",
       "      <td>-0.304177</td>\n",
       "      <td>0.010149</td>\n",
       "      <td>-0.105604</td>\n",
       "      <td>0.100195</td>\n",
       "      <td>0.208448</td>\n",
       "      <td>-0.110721</td>\n",
       "      <td>0.093812</td>\n",
       "      <td>0.046361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like</th>\n",
       "      <td>0.029598</td>\n",
       "      <td>-0.215566</td>\n",
       "      <td>-0.159101</td>\n",
       "      <td>0.045574</td>\n",
       "      <td>-0.279257</td>\n",
       "      <td>-0.001587</td>\n",
       "      <td>-0.045344</td>\n",
       "      <td>0.047919</td>\n",
       "      <td>-0.217635</td>\n",
       "      <td>-0.028089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040511</td>\n",
       "      <td>0.226621</td>\n",
       "      <td>-0.286857</td>\n",
       "      <td>-0.070956</td>\n",
       "      <td>0.037725</td>\n",
       "      <td>0.028874</td>\n",
       "      <td>0.054807</td>\n",
       "      <td>-0.283012</td>\n",
       "      <td>0.012261</td>\n",
       "      <td>-0.113263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>one</th>\n",
       "      <td>0.138026</td>\n",
       "      <td>-0.136071</td>\n",
       "      <td>-0.169180</td>\n",
       "      <td>-0.106101</td>\n",
       "      <td>0.049800</td>\n",
       "      <td>0.143565</td>\n",
       "      <td>-0.202722</td>\n",
       "      <td>-0.038723</td>\n",
       "      <td>-0.055654</td>\n",
       "      <td>-0.037901</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.226241</td>\n",
       "      <td>-0.128330</td>\n",
       "      <td>-0.087331</td>\n",
       "      <td>-0.004598</td>\n",
       "      <td>-0.062975</td>\n",
       "      <td>0.081216</td>\n",
       "      <td>0.141682</td>\n",
       "      <td>-0.131175</td>\n",
       "      <td>-0.052328</td>\n",
       "      <td>0.050949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "‘     0.281076 -0.181313  0.253112 -0.122151  0.152994 -0.095516 -0.029143   \n",
       "said  0.119032 -0.598810  0.103481 -0.323416  0.037999  0.091895 -0.107412   \n",
       "neel -0.237389 -0.141943 -0.566874 -0.403153 -0.492787 -0.356656  0.072839   \n",
       "debu -0.265364 -0.161909 -0.280103 -0.210716 -0.160269  0.242691 -0.019847   \n",
       "like  0.029598 -0.215566 -0.159101  0.045574 -0.279257 -0.001587 -0.045344   \n",
       "one   0.138026 -0.136071 -0.169180 -0.106101  0.049800  0.143565 -0.202722   \n",
       "\n",
       "            7         8         9     ...           90        91        92  \\\n",
       "‘     0.377922  0.280936 -0.166578    ...    -0.027879 -0.259911 -0.394983   \n",
       "said -0.134377 -0.170499 -0.045597    ...    -0.846326 -0.249982  0.531245   \n",
       "neel -0.228308 -0.123777 -0.424938    ...    -0.072097  1.043128 -0.046427   \n",
       "debu  0.310536 -0.126590 -0.137395    ...    -0.122794 -0.123167 -0.304177   \n",
       "like  0.047919 -0.217635 -0.028089    ...     0.040511  0.226621 -0.286857   \n",
       "one  -0.038723 -0.055654 -0.037901    ...    -0.226241 -0.128330 -0.087331   \n",
       "\n",
       "            93        94        95        96        97        98        99  \n",
       "‘    -0.462198 -0.138003 -0.036760  0.360835  0.145279 -0.347896 -0.358649  \n",
       "said  0.477507 -0.057289  0.094422  0.100349 -0.397421 -0.038629 -0.398164  \n",
       "neel  0.276616  0.250141  0.135944 -0.296210  0.035597 -0.020644 -0.271495  \n",
       "debu  0.010149 -0.105604  0.100195  0.208448 -0.110721  0.093812  0.046361  \n",
       "like -0.070956  0.037725  0.028874  0.054807 -0.283012  0.012261 -0.113263  \n",
       "one  -0.004598 -0.062975  0.081216  0.141682 -0.131175 -0.052328  0.050949  \n",
       "\n",
       "[6 rows x 100 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = cbow.get_weights()[0]\n",
    "weights = weights[1:]\n",
    "\n",
    "print(weights.shape)\n",
    "\n",
    "import pandas as pd\n",
    "wrd_emb = pd.DataFrame(weights , index = list(id2word.values())[1:])\n",
    "wrd_emb.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
