{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import cv2\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import json\n",
    "from time import time\n",
    "import pickle\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.resnet50 import ResNet50,preprocess_input,decode_predictions,decode_predictions\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model,load_model\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input,Dense,Dropout,Embedding,LSTM\n",
    "from keras.layers.merge import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_dict(path):\n",
    "    train = None\n",
    "    with open(path) as f:\n",
    "        train = f.read()\n",
    "\n",
    "    json_accept_str = train.replace(\"'\",\"\\\"\")\n",
    "#     json_accept_str = json_accept_str.replace(\";\",\"\\\"\")\n",
    "    train = json.loads(json_accept_str)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_Cap_map = str_to_dict(\"train_img_cap_map.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word_to_idx = str_to_dict(\"word_to_idx.txt\")\n",
    "# len(train_img_Cap_map)\n",
    "# idx_to_word = str_to_dict(\"idx_to_word.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size 1848\n"
     ]
    }
   ],
   "source": [
    "with open(\"total_word.txt\",\"r\") as f:\n",
    "    total_words = f.read()\n",
    "with open(\"vocab.txt\",\"r\") as f:\n",
    "    vocab = f.read()\n",
    "    \n",
    "total_words = [wrd[2:-1] for wrd in total_words.split(\",\")]\n",
    "vocab = [wrd[2:-1] for wrd in vocab.split(\",\")]\n",
    "\n",
    "word_to_idx = {}\n",
    "idx_to_word = {}\n",
    "\n",
    "for i,word in enumerate(total_words):\n",
    "    word_to_idx[word] = i+1\n",
    "    idx_to_word[i+1] = word\n",
    "# Two special words\n",
    "idx_to_word[1846] = 'ss'\n",
    "word_to_idx['ss'] = 1846\n",
    "\n",
    "idx_to_word[1847] = 'es'\n",
    "word_to_idx['es'] = 1847\n",
    "\n",
    "vocab_size = len(word_to_idx) + 1\n",
    "print(\"Vocab Size\",vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(train_img_Cap_map,encoding_train,word_to_idx,max_len,batch_size):\n",
    "    X1,X2, y = [],[],[]\n",
    "    \n",
    "    n =0\n",
    "    while True:\n",
    "        for key,desc_list in train_img_Cap_map.items():\n",
    "            n += 1\n",
    "            \n",
    "            photo = encoding_train[key]\n",
    "            for desc in desc_list:\n",
    "                \n",
    "                seq = [word_to_idx[word] for word in desc.split() if word in word_to_idx]\n",
    "                for i in range(1,len(seq)):\n",
    "                    xi = seq[0:i]\n",
    "                    yi = seq[i]\n",
    "                    \n",
    "                    #0 denote padding word\n",
    "                    xi = pad_sequences([xi],maxlen=max_len,value=0,padding='post')[0]\n",
    "                    yi = to_categorical([yi],num_classes=vocab_size)[0]\n",
    "                    \n",
    "                    X1.append(photo)\n",
    "                    X2.append(xi)\n",
    "                    y.append(yi)\n",
    "                    \n",
    "                if n==batch_size:\n",
    "                    yield [[np.array(X1),np.array(X2)],np.array(y)]\n",
    "                    X1,X2,y = [],[],[]\n",
    "                    n = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Encoding/encoded_train_features.pkl\",\"rb\") as f:\n",
    "    encoding_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Encoding/encoded_test_features.pkl\",\"rb\") as f:\n",
    "    encoding_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(encoding_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"glove.6B.50d.txt\",encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    \n",
    "    word = values[0]\n",
    "    word_embedding = np.array(values[1:],dtype='float')\n",
    "    embedding_index[word] = word_embedding\n",
    "f.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.063795 , -0.12926  ,  0.57822  ,  0.091202 , -0.49303  ,\n",
       "       -1.1864   , -0.33771  ,  0.098739 ,  0.36918  , -0.081918 ,\n",
       "       -0.38308  ,  0.46332  , -0.6848   , -0.082209 , -0.20556  ,\n",
       "        0.28582  ,  0.72209  , -0.0068532,  0.61001  , -0.017818 ,\n",
       "       -0.4877   , -0.22869  , -1.4502   ,  0.91573  ,  0.17179  ,\n",
       "        1.0807   ,  0.61252  , -0.20502  , -0.37131  ,  0.30343  ,\n",
       "       -1.0234   , -0.38941  ,  1.1749   ,  0.1327   , -0.22279  ,\n",
       "        0.0060945, -0.28316  , -0.46707  ,  0.11977  , -0.19949  ,\n",
       "        0.38805  ,  0.056067 , -0.36484  ,  0.61864  , -0.11445  ,\n",
       "        0.17343  ,  0.52787  , -0.35576  , -0.41406  ,  0.70935  ])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding_index['appe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    emb_dim = 50\n",
    "    matrix = np.zeros((vocab_size,emb_dim))\n",
    "    for word,idx in word_to_idx.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            matrix[idx] = embedding_vector\n",
    "            \n",
    "    return matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1848, 50)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = get_embedding_matrix()\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_():\n",
    "    input_img_features = Input(shape=(2048,))\n",
    "    inp_img1 = Dropout(0.3)(input_img_features)\n",
    "    inp_img2 = Dense(256,activation='relu')(inp_img1)\n",
    "    # Captions as Input\n",
    "    input_captions = Input(shape=(35,))\n",
    "    inp_cap1 = Embedding(input_dim=vocab_size,output_dim=50,mask_zero=True)(input_captions)\n",
    "    inp_cap2 = Dropout(0.3)(inp_cap1)\n",
    "    inp_cap3 = LSTM(256)(inp_cap2)\n",
    "    decoder1 = add([inp_img2,inp_cap3])\n",
    "    decoder2 = Dense(256,activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size,activation='softmax')(decoder2)\n",
    "\n",
    "    # Combined Model\n",
    "    model = Model(inputs=[input_img_features,input_captions],outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            (None, 35)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 2048)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 35, 50)       92400       input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 2048)         0           input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 35, 50)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 256)          524544      dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 256)          314368      dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 256)          0           dense_8[0][0]                    \n",
      "                                                                 lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 256)          65792       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 1848)         474936      dense_9[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,472,040\n",
      "Trainable params: 1,472,040\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model_()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Thing - Embedding Layer\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer=\"adam\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "max_len = 35\n",
    "batch_size = 50\n",
    "number_pics_per_batch = 2000\n",
    "steps = len(train_img_Cap_map)//number_pics_per_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch = 2): \n",
    "    for i in range(epochs):\n",
    "        generator = data_generator(train_img_Cap_map,encoding_train,word_to_idx,max_len,batch_size)\n",
    "        model.fit_generator(generator,epochs=1,steps_per_epoch=steps,verbose=1)\n",
    "        model.save('./model_weights/model_'+str(i)+'.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "3/3 [==============================] - 51s 17s/step - loss: 8.2971\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 7.1010\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 5.7436\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 5.6137\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 5.2861\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 5.0469\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.8938\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.7865\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.6792\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.5798\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 11s 4s/step - loss: 4.4916\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 11s 4s/step - loss: 4.3922\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.3027\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.2102\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.1098\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 4.0104\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 3.9091\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 3.8089\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 3.7069\n",
      "Epoch 1/1\n",
      "3/3 [==============================] - 10s 3s/step - loss: 3.6055\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
